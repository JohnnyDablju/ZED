{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RE_SPACE = re.compile(\"\\s+\")\n",
    "RE_HASHTAG = re.compile(\"[@#][_a-z0-9]+\")\n",
    "RE_EMOTICON = re.compile(\"(:-?\\))|(:p)|(:d+)|(:-?\\()|(:/)|(;-?\\))|(<3)|(=\\))|(\\)-?:)|(:'\\()|(8\\))\")\n",
    "RE_HTTP = re.compile(\"http(s)?://[/\\.a-z0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text = text.strip().lower()\n",
    "    text = text.replace('&nbsp;', ' ')\n",
    "    text = text.replace('&lt;', '<')\n",
    "    text = text.replace('&gt;', '>')\n",
    "    text = text.replace('&amp;', '&')\n",
    "    text = text.replace('&pound;', u'£')\n",
    "    text = text.replace('&euro;', u'€')\n",
    "    text = text.replace('&copy;', u'©')\n",
    "    text = text.replace('&reg;', u'®')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "    text = tokenizer.tokenize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preStemClean(text):\n",
    "    words = []\n",
    "    for word in text:\n",
    "        word = word.replace('#', '')\n",
    "        word = word.replace('@', '')\n",
    "        if RE_HTTP.match(word) == None:\n",
    "            words.append(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem(text):\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    words = []\n",
    "    for word in text:\n",
    "        words.append(stemmer.stem(word))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def postStemClean(text):\n",
    "    #stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stopwords = [\"a\", \"about\", \"after\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\",\n",
    "            \"before\", \"being\", \"between\", \"both\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"during\", \"each\",\n",
    "            \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\",\n",
    "            \"himself\", \"his\", \"how\", \"i\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"let\", \"me\", \"more\", \"most\", \"my\",\n",
    "            \"myself\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"own\", \"sha\",\n",
    "            \"she\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\",\n",
    "            \"then\", \"there\", \"there's\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"until\", \"up\", \"very\",\n",
    "            \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\",\"whom\", \"with\", \"would\", \"you\",\n",
    "            \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
    "            \"n't\", \"'s\", \"'ll\", \"'re\", \"'d\", \"'m\", \"'ve\",\n",
    "            \"above\", \"again\", \"against\", \"below\", \"but\", \n",
    "                 #\"cannot\", \n",
    "            \"down\", \"few\", \"if\", \"no\", \"nor\", \n",
    "                 #\"not\", \n",
    "            \"off\", \"out\", \"over\", \"same\", \"too\", \"under\", \n",
    "                 #\"why\"\n",
    "            \"may\"\n",
    "                ]\n",
    "    punctuation = ['.', ',', ';', ':', '&', '-', '->', '/', '\\\\']\n",
    "    words = []\n",
    "    for word in text:\n",
    "        if word not in stopwords and word not in punctuation:\n",
    "            words.append(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def prepareTweets(documents):\n",
    "    wordCounter = Counter()\n",
    "    tweets = []\n",
    "    for i in documents.index:\n",
    "        tweet = normalize(documents[i])\n",
    "        tweet = tokenize(tweet)\n",
    "        tweet = preStemClean(tweet)\n",
    "        tweet = stem(tweet)\n",
    "        # moze lematyzacja?\n",
    "        tweet = postStemClean(tweet)\n",
    "        wordCounter.update(tweet)\n",
    "        tweets.append(tweet)\n",
    "    return tweets, wordCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFeatureDictionary(wordCounter, minWordCount):\n",
    "    commonWords = list([k for k, v in wordCounter.most_common() if v > minWordCount])\n",
    "    dictionary = {}\n",
    "    for word in commonWords:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "def createBagOfWords(tweets, features, inputLabels):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    outputLabels = []\n",
    "    for i in range(len(tweets)):\n",
    "        if not inputLabels.empty:\n",
    "            outputLabels.append(inputLabels[i])\n",
    "        tokens = tweets[i]\n",
    "        \n",
    "        for token in set(tokens):\n",
    "            if token not in features:\n",
    "                continue\n",
    "            row.append(i)\n",
    "            col.append(features[token])\n",
    "            data.append(1)\n",
    "    return csr_matrix((data, (row, col)), shape=(len(tweets), len(features))), outputLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================== Results ===================\n",
      "            Positive    Neutral     Negative   \n",
      "F1        [ 0.95686275  0.95060373  0.95659377  1.        ]\n",
      "Precision [ 0.99846547  1.          0.91709022  1.        ]\n",
      "Recall    [ 0.91858824  0.90585774  0.99965374  1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "trainFile = pd.read_csv('train.csv', sep=',')\n",
    "preparedTweets, wordsCounter = prepareTweets(trainFile.iloc[:, 2])\n",
    "featureDictionary = getFeatureDictionary(wordsCounter, 1)\n",
    "xTrain, yTrain = createBagOfWords(preparedTweets, featureDictionary, trainFile.iloc[:, 1])\n",
    "labels = list(set(yTrain))\n",
    "classifier = RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=13)\n",
    "classifier.fit(xTrain, yTrain)\n",
    "predicted = classifier.predict(xTrain)\n",
    "\n",
    "print(\"=================== Results ===================\")\n",
    "print(\"            Positive    Neutral     Negative   \")\n",
    "print(\"F1       \", f1_score(yTrain, predicted, average=None, pos_label=None, labels=labels))\n",
    "print(\"Precision\", precision_score(yTrain, predicted, average=None, pos_label=None, labels=labels))\n",
    "print(\"Recall   \", recall_score(yTrain, predicted, average=None, pos_label=None, labels=labels))\n",
    "\n",
    "testFile = pd.read_csv('test.csv', sep=',')\n",
    "preparedTweets, x = prepareTweets(testFile.iloc[:, 1])\n",
    "xTest, yTest = createBagOfWords(preparedTweets, featureDictionary, pd.DataFrame())\n",
    "predicted = classifier.predict(xTest)\n",
    "\n",
    "submission = pd.concat([testFile.iloc[:,  0], pd.DataFrame({'Category': predicted})], axis=1)\n",
    "submission.to_csv('submission.csv', sep=',', index = False, index_label = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
